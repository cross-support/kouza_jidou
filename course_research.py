#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¬›åº§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è‡ªå‹•ãƒªã‚µãƒ¼ãƒãƒ„ãƒ¼ãƒ« - å·¥ç¨‹1A: Googleæ¤œç´¢ã«ã‚ˆã‚‹ãƒãƒƒãƒˆæƒ…å ±åé›†

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€è¬›åº§ã®ãƒ†ãƒ¼ãƒã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’Webä¸Šã‹ã‚‰è‡ªå‹•åé›†ã—ã¾ã™ã€‚
ãƒ–ãƒ­ã‚°è‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®å·¥ç¨‹5ï¼ˆä¸€æ¬¡æƒ…å ±è¿½åŠ ï¼‰ã‚’å‚è€ƒã«ã€è¬›åº§ä½œæˆå‘ã‘ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ã¦ã„ã¾ã™ã€‚
"""

import argparse
import json
import sys
import time
from datetime import datetime
from typing import List, Dict, Optional
import re

try:
    import requests
    from bs4 import BeautifulSoup
    from duckduckgo_search import DDGS
except ImportError as e:
    print(f"Error: Required library not found: {e}", file=sys.stderr)
    print("Please install required libraries:", file=sys.stderr)
    print("  pip3 install requests beautifulsoup4 duckduckgo-search", file=sys.stderr)
    sys.exit(1)


class CourseResearcher:
    """è¬›åº§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒªã‚µãƒ¼ãƒã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, keywords: List[str], num_results: int = 10):
        """
        åˆæœŸåŒ–

        Args:
            keywords: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
            num_results: å–å¾—ã™ã‚‹æ¤œç´¢çµæœã®æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰
        """
        self.keywords = keywords
        self.num_results = num_results
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.collected_data = []

    def search_duckduckgo(self, query: str) -> List[Dict[str, str]]:
        """
        DuckDuckGoæ¤œç´¢ã‚’å®Ÿè¡Œã—ã€æ¤œç´¢çµæœã®URLã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª

        Returns:
            æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ [{'title': str, 'url': str, 'snippet': str}, ...]
        """
        print(f"\nğŸ” DuckDuckGoæ¤œç´¢: '{query}'")

        try:
            results = []

            # DuckDuckGoæ¤œç´¢ã‚’å®Ÿè¡Œ
            with DDGS() as ddgs:
                search_results = ddgs.text(
                    query + " site:.jp OR site:.com",  # æ—¥æœ¬èªã‚µã‚¤ãƒˆã¨è‹±èªã‚µã‚¤ãƒˆã‚’å„ªå…ˆ
                    region='jp-jp',
                    safesearch='moderate',
                    max_results=self.num_results
                )

                for result in search_results:
                    results.append({
                        'title': result.get('title', 'No title'),
                        'url': result.get('href', ''),
                        'snippet': result.get('body', '')
                    })

            print(f"  âœ“ {len(results)}ä»¶ã®æ¤œç´¢çµæœã‚’å–å¾—")
            return results

        except Exception as e:
            print(f"  âœ— æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
            return []

    def extract_content(self, url: str) -> Optional[Dict[str, str]]:
        """
        æŒ‡å®šURLã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º

        Args:
            url: å¯¾è±¡URL

        Returns:
            æŠ½å‡ºã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ {'title': str, 'text': str, 'url': str}
        """
        try:
            print(f"  ğŸ“„ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—: {url[:60]}...")

            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()

            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            title = soup.find('title')
            title_text = title.get_text().strip() if title else 'No title'

            # æœ¬æ–‡ã‚’å–å¾—ï¼ˆarticle > main > body ã®é †ã§è©¦è¡Œï¼‰
            content = None
            for tag in ['article', 'main', 'body']:
                content = soup.find(tag)
                if content:
                    break

            if not content:
                content = soup

            # ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            text = content.get_text(separator='\n', strip=True)

            # ç©ºç™½è¡Œã‚’å‰Šé™¤ã—ã€æ•´å½¢
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            cleaned_text = '\n'.join(lines)

            # æ–‡å­—æ•°åˆ¶é™ï¼ˆæœ€å¤§10000æ–‡å­—ï¼‰
            if len(cleaned_text) > 10000:
                cleaned_text = cleaned_text[:10000] + '...'

            print(f"    âœ“ {len(cleaned_text)}æ–‡å­—ã‚’æŠ½å‡º")

            return {
                'title': title_text,
                'url': url,
                'text': cleaned_text,
                'word_count': len(cleaned_text)
            }

        except Exception as e:
            print(f"    âœ— ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
            return None

    def research_from_urls(self, url_list: List[str]) -> Dict:
        """
        URLãƒªã‚¹ãƒˆã‹ã‚‰ãƒªã‚µãƒ¼ãƒã‚’å®Ÿè¡Œ

        Args:
            url_list: URLã®ãƒªã‚¹ãƒˆ

        Returns:
            åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸
        """
        print("=" * 60)
        print("ğŸ“ è¬›åº§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è‡ªå‹•ãƒªã‚µãƒ¼ãƒ - å·¥ç¨‹1Aï¼ˆURLãƒªã‚¹ãƒˆæ–¹å¼ï¼‰")
        print("=" * 60)
        print(f"ğŸ“‹ å¯¾è±¡URLæ•°: {len(url_list)}")

        all_results = []

        for i, url in enumerate(url_list, 1):
            print(f"\n[{i}/{len(url_list)}] å‡¦ç†ä¸­...")

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆ1ç§’å¾…æ©Ÿï¼‰
            if i > 1:
                time.sleep(1)

            content = self.extract_content(url)
            if content:
                all_results.append(content)

        # çµæœã‚’ã¾ã¨ã‚ã‚‹
        research_data = {
            'research_date': datetime.now().isoformat(),
            'source_type': 'url_list',
            'total_sources': len(all_results),
            'success_rate': f"{len(all_results)}/{len(url_list)} ({len(all_results)/len(url_list)*100:.1f}%)",
            'sources': all_results,
            'summary': {
                'total_words': sum(s['word_count'] for s in all_results),
                'unique_urls': len(set(s['url'] for s in all_results)),
                'average_words_per_source': sum(s['word_count'] for s in all_results) // len(all_results) if all_results else 0
            }
        }

        return research_data

    def research(self) -> Dict:
        """
        ãƒªã‚µãƒ¼ãƒã‚’å®Ÿè¡Œï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢æ–¹å¼ï¼‰

        Returns:
            åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸
        """
        print("=" * 60)
        print("ğŸ“ è¬›åº§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è‡ªå‹•ãƒªã‚µãƒ¼ãƒ - å·¥ç¨‹1A")
        print("=" * 60)

        all_results = []

        for keyword in self.keywords:
            # DuckDuckGoæ¤œç´¢ã‚’å®Ÿè¡Œ
            search_results = self.search_duckduckgo(keyword)

            # å„æ¤œç´¢çµæœã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            for result in search_results:
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆ1ç§’å¾…æ©Ÿï¼‰
                time.sleep(1)

                content = self.extract_content(result['url'])
                if content:
                    content['search_keyword'] = keyword
                    content['search_snippet'] = result['snippet']
                    all_results.append(content)

        # çµæœã‚’ã¾ã¨ã‚ã‚‹
        research_data = {
            'research_date': datetime.now().isoformat(),
            'source_type': 'keyword_search',
            'keywords': self.keywords,
            'total_sources': len(all_results),
            'sources': all_results,
            'summary': {
                'total_words': sum(s['word_count'] for s in all_results),
                'unique_urls': len(set(s['url'] for s in all_results))
            }
        }

        return research_data

    def save_to_json(self, data: Dict, output_path: str):
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã§ä¿å­˜

        Args:
            data: ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿
            output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
        except Exception as e:
            print(f"\nâœ— ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)


def main():
    parser = argparse.ArgumentParser(
        description='è¬›åº§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è‡ªå‹•ãƒªã‚µãƒ¼ãƒãƒ„ãƒ¼ãƒ« - Webæƒ…å ±åé›†'
    )

    # URLãƒªã‚¹ãƒˆæ–¹å¼ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢æ–¹å¼ã®é¸æŠ
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument(
        '--url-list',
        help='URLãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆ1è¡Œã«1URLï¼‰'
    )
    input_group.add_argument(
        '--keywords',
        nargs='+',
        help='æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°æŒ‡å®šå¯ï¼‰ä¾‹: "ChatGPT æ¥­å‹™æ´»ç”¨" "AI æ´»ç”¨äº‹ä¾‹"'
    )

    parser.add_argument(
        '--num-results',
        type=int,
        default=10,
        help='å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§å–å¾—ã™ã‚‹æ¤œç´¢çµæœã®æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰â€»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢æ™‚ã®ã¿'
    )
    parser.add_argument(
        '--output',
        default='course_research_output.json',
        help='å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: course_research_output.jsonï¼‰'
    )

    args = parser.parse_args()

    # URLãƒªã‚¹ãƒˆæ–¹å¼
    if args.url_list:
        try:
            with open(args.url_list, 'r', encoding='utf-8') as f:
                url_list = [line.strip() for line in f if line.strip() and not line.startswith('#')]

            if not url_list:
                print("Error: URLãƒªã‚¹ãƒˆãŒç©ºã§ã™", file=sys.stderr)
                sys.exit(1)

            researcher = CourseResearcher(keywords=[], num_results=0)
            research_data = researcher.research_from_urls(url_list)

        except FileNotFoundError:
            print(f"Error: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.url_list}", file=sys.stderr)
            sys.exit(1)
        except Exception as e:
            print(f"Error: {e}", file=sys.stderr)
            sys.exit(1)

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢æ–¹å¼
    else:
        researcher = CourseResearcher(
            keywords=args.keywords,
            num_results=args.num_results
        )
        research_data = researcher.research()

    # çµæœã‚’ä¿å­˜
    researcher.save_to_json(research_data, args.output)

    # ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    print("\n" + "=" * 60)
    print("ğŸ“Š ãƒªã‚µãƒ¼ãƒå®Œäº†ã‚µãƒãƒªãƒ¼")
    print("=" * 60)

    if args.url_list:
        print(f"å…¥åŠ›æ–¹å¼: URLãƒªã‚¹ãƒˆ ({args.url_list})")
        print(f"æˆåŠŸç‡: {research_data['success_rate']}")
    else:
        print(f"å…¥åŠ›æ–¹å¼: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢")
        print(f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: {len(args.keywords)}")

    print(f"å–å¾—ã‚½ãƒ¼ã‚¹æ•°: {research_data['total_sources']}")
    print(f"ç·æ–‡å­—æ•°: {research_data['summary']['total_words']:,}æ–‡å­—")
    print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯URLæ•°: {research_data['summary']['unique_urls']}")

    if args.url_list and research_data['total_sources'] > 0:
        print(f"å¹³å‡æ–‡å­—æ•°: {research_data['summary']['average_words_per_source']:,}æ–‡å­—/ã‚½ãƒ¼ã‚¹")

    print("=" * 60)


if __name__ == '__main__':
    main()
