#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¬›åº§ç”¨èªåˆ†æãƒ„ãƒ¼ãƒ« - å·¥ç¨‹1D

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é‡è¦ãªç”¨èªã‚’æŠ½å‡ºã—ã€
å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã¨ç”¨èªç¶²ç¾…æ€§ã‚’åˆ†æã—ã¾ã™ã€‚

SEO_AI_ver6.5ã®å·¥ç¨‹3Bï¼ˆå…±èµ·èªæŠ½å‡ºï¼‰ã‚’è¬›åº§è‡ªå‹•åŒ–ã«é©å¿œã€‚
"""

import argparse
import json
import re
import sys
from collections import Counter
from datetime import datetime
from typing import Dict, List, Set


class CourseTerminologyAnalyzer:
    """è¬›åº§ç”¨èªåˆ†æã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.analysis_date = datetime.now().isoformat()

        # ä¸€èˆ¬çš„ãªã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆé™¤å¤–ã™ã‚‹å˜èªï¼‰
        self.stopwords = set([
            'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã‚Œ', 'ã“ã®', 'ãã®', 'ã‚ã®',
            'ã“ã¨', 'ã‚‚ã®', 'ãŸã‚', 'ãªã©', 'ã“ã“', 'ãã“', 'ã‚ãã“',
            'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
            'can', 'could', 'may', 'might', 'must', 'shall', 'should',
            'this', 'that', 'these', 'those', 'and', 'or', 'but', 'not'
        ])

    def extract_terminology_from_web(self, web_data: Dict) -> Dict:
        """
        Webãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç”¨èªã‚’æŠ½å‡º

        Args:
            web_data: Webãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿

        Returns:
            æŠ½å‡ºã•ã‚ŒãŸç”¨èªã®è¾æ›¸
        """
        if not web_data or not web_data.get('sources'):
            return {'status': 'no_data', 'terms': []}

        all_text = []
        for source in web_data.get('sources', []):
            content = source.get('content', '')
            title = source.get('title', '')
            all_text.append(title + ' ' + content)

        combined_text = ' '.join(all_text)

        # ç”¨èªæŠ½å‡º
        terms = self._extract_terms(combined_text)

        return {
            'status': 'extracted',
            'source_type': 'web',
            'terms': terms,
            'total_unique_terms': len(terms)
        }

    def extract_terminology_from_youtube(self, youtube_data: Dict) -> Dict:
        """
        YouTubeæ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç”¨èªã‚’æŠ½å‡º

        Args:
            youtube_data: YouTubeæ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿

        Returns:
            æŠ½å‡ºã•ã‚ŒãŸç”¨èªã®è¾æ›¸
        """
        if not youtube_data or not youtube_data.get('transcriptions'):
            return {'status': 'no_data', 'terms': []}

        all_text = []
        for transcript in youtube_data.get('transcriptions', []):
            text = transcript.get('text', '')
            all_text.append(text)

        combined_text = ' '.join(all_text)

        # ç”¨èªæŠ½å‡º
        terms = self._extract_terms(combined_text)

        return {
            'status': 'extracted',
            'source_type': 'youtube',
            'terms': terms,
            'total_unique_terms': len(terms)
        }

    def _extract_terms(self, text: str) -> List[Dict]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ãªç”¨èªã‚’æŠ½å‡º

        Args:
            text: åˆ†æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            ç”¨èªã®ãƒªã‚¹ãƒˆ
        """
        # è‹±æ•°å­—ã‚’å«ã‚€2æ–‡å­—ä»¥ä¸Šã®å˜èªã‚’æŠ½å‡º
        words = re.findall(r'\b[A-Za-z0-9ã‚¡-ãƒ´ãƒ¼]{2,}\b|[ä¸€-é¾¯]{2,}', text)

        # é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
        word_freq = Counter(words)

        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å¤–
        filtered_words = {
            word: freq for word, freq in word_freq.items()
            if word.lower() not in self.stopwords and freq >= 2
        }

        # é »åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_terms = sorted(
            filtered_words.items(),
            key=lambda x: x[1],
            reverse=True
        )

        # ä¸Šä½50ä»¶ã‚’å–å¾—
        top_terms = []
        for term, frequency in sorted_terms[:50]:
            top_terms.append({
                'term': term,
                'frequency': frequency,
                'category': self._categorize_term(term),
                'learning_phase': None  # å¾Œã§å‰²ã‚Šå½“ã¦
            })

        return top_terms

    def _categorize_term(self, term: str) -> str:
        """
        ç”¨èªã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡

        Args:
            term: ç”¨èª

        Returns:
            ã‚«ãƒ†ã‚´ãƒªå
        """
        # æŠ€è¡“ç”¨èªã®åˆ¤å®š
        tech_patterns = [
            r'AI', r'API', r'ChatGPT', r'GPT', r'LLM', r'DX', r'IT',
            r'ã‚·ã‚¹ãƒ†ãƒ ', r'ãƒ—ãƒ­ã‚°ãƒ©ãƒ ', r'ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ', r'ãƒ‡ãƒ¼ã‚¿',
            r'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯', r'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£', r'ã‚¯ãƒ©ã‚¦ãƒ‰'
        ]

        for pattern in tech_patterns:
            if re.search(pattern, term, re.IGNORECASE):
                return 'technical'

        # ãƒ“ã‚¸ãƒã‚¹ç”¨èªã®åˆ¤å®š
        business_patterns = [
            r'æ¥­å‹™', r'åŠ¹ç‡', r'ç”Ÿç”£æ€§', r'ã‚³ã‚¹ãƒˆ', r'å£²ä¸Š', r'åˆ©ç›Š',
            r'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', r'å–¶æ¥­', r'ç®¡ç†', r'æˆ¦ç•¥', r'çµŒå–¶'
        ]

        for pattern in business_patterns:
            if re.search(pattern, term, re.IGNORECASE):
                return 'business'

        # æ•™è‚²ãƒ»å­¦ç¿’ç”¨èªã®åˆ¤å®š
        learning_patterns = [
            r'å­¦ç¿’', r'æ•™è‚²', r'ç ”ä¿®', r'ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°', r'ã‚¹ã‚­ãƒ«',
            r'çŸ¥è­˜', r'ç†è§£', r'ç¿’å¾—', r'å®Ÿè·µ'
        ]

        for pattern in learning_patterns:
            if re.search(pattern, term, re.IGNORECASE):
                return 'learning'

        return 'general'

    def map_to_learning_phases(self, terms: List[Dict], course_theme: str = None) -> List[Dict]:
        """
        ç”¨èªã‚’å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºã«ãƒãƒƒãƒ”ãƒ³ã‚°

        Args:
            terms: ç”¨èªãƒªã‚¹ãƒˆ
            course_theme: è¬›åº§ãƒ†ãƒ¼ãƒï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºãŒãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚ŒãŸç”¨èªãƒªã‚¹ãƒˆ
        """
        # å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º:
        # 1. å°å…¥ï¼ˆIntroductionï¼‰: åŸºæœ¬æ¦‚å¿µã€å®šç¾©ã€èƒŒæ™¯
        # 2. ç†è§£ï¼ˆUnderstandingï¼‰: ä»•çµ„ã¿ã€åŸç†ã€è©³ç´°
        # 3. å®Ÿè·µï¼ˆApplicationï¼‰: ä½¿ã„æ–¹ã€æ´»ç”¨æ³•ã€äº‹ä¾‹

        for term_dict in terms:
            term = term_dict['term']
            category = term_dict['category']

            # ã‚«ãƒ†ã‚´ãƒªã¨é »åº¦ã«åŸºã¥ã„ã¦å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºã‚’æ¨å®š
            if category == 'learning' or 'åŸºæœ¬' in term or 'æ¦‚è¦' in term or 'å…¥é–€' in term:
                phase = 'introduction'
            elif 'æ–¹æ³•' in term or 'ä½¿ã„æ–¹' in term or 'æ´»ç”¨' in term or 'å®Ÿè·µ' in term or 'äº‹ä¾‹' in term:
                phase = 'application'
            else:
                phase = 'understanding'

            term_dict['learning_phase'] = phase

        return terms

    def generate_terminology_report(
        self,
        web_terms: Dict,
        youtube_terms: Dict,
        course_theme: str = None
    ) -> Dict:
        """
        çµ±åˆç”¨èªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ

        Args:
            web_terms: Webç”¨èªæŠ½å‡ºçµæœ
            youtube_terms: YouTubeç”¨èªæŠ½å‡ºçµæœ
            course_theme: è¬›åº§ãƒ†ãƒ¼ãƒï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            çµ±åˆç”¨èªåˆ†æãƒ¬ãƒãƒ¼ãƒˆ
        """
        # ç”¨èªã‚’çµ±åˆ
        all_terms = []

        if web_terms.get('status') == 'extracted':
            all_terms.extend(web_terms.get('terms', []))

        if youtube_terms.get('status') == 'extracted':
            all_terms.extend(youtube_terms.get('terms', []))

        # é‡è¤‡ã‚’çµ±åˆï¼ˆåŒã˜ç”¨èªã®é »åº¦ã‚’åˆç®—ï¼‰
        term_dict = {}
        for term_info in all_terms:
            term = term_info['term']
            if term in term_dict:
                term_dict[term]['frequency'] += term_info['frequency']
                term_dict[term]['sources'] = term_dict[term].get('sources', []) + [term_info.get('source_type', 'unknown')]
            else:
                term_dict[term] = term_info.copy()
                term_dict[term]['sources'] = [term_info.get('source_type', 'unknown')]

        # çµ±åˆç”¨èªãƒªã‚¹ãƒˆ
        integrated_terms = list(term_dict.values())

        # é »åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        integrated_terms.sort(key=lambda x: x['frequency'], reverse=True)

        # ä¸Šä½30ä»¶ã«çµã‚‹
        top_terms = integrated_terms[:30]

        # å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºã«ãƒãƒƒãƒ”ãƒ³ã‚°
        top_terms = self.map_to_learning_phases(top_terms, course_theme)

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = {
            'analysis_date': self.analysis_date,
            'course_theme': course_theme,
            'terminology_summary': {
                'total_unique_terms': len(integrated_terms),
                'top_terms_count': len(top_terms),
                'categories': self._count_categories(top_terms),
                'learning_phases': self._count_phases(top_terms)
            },
            'top_terms': top_terms,
            'recommendations': self._generate_terminology_recommendations(top_terms)
        }

        return report

    def _count_categories(self, terms: List[Dict]) -> Dict:
        """ç”¨èªã®ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚«ã‚¦ãƒ³ãƒˆ"""
        categories = Counter(term['category'] for term in terms)
        return dict(categories)

    def _count_phases(self, terms: List[Dict]) -> Dict:
        """ç”¨èªã®å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ã‚«ã‚¦ãƒ³ãƒˆ"""
        phases = Counter(term.get('learning_phase', 'unknown') for term in terms)
        return dict(phases)

    def _generate_terminology_recommendations(self, terms: List[Dict]) -> List[str]:
        """ç”¨èªåˆ†æçµæœã‹ã‚‰æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []

        # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã‚’ç¢ºèª
        categories = self._count_categories(terms)
        phases = self._count_phases(terms)

        # æŠ€è¡“ç”¨èªãŒå¤šã„
        if categories.get('technical', 0) > len(terms) * 0.5:
            recommendations.append(
                "ğŸ’¡ æŠ€è¡“ç”¨èªãŒå¤šãæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚åˆå­¦è€…å‘ã‘ã«ç”¨èªè§£èª¬ã‚’å……å®Ÿã•ã›ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
            )

        # ãƒ“ã‚¸ãƒã‚¹ç”¨èªãŒå¤šã„
        if categories.get('business', 0) > len(terms) * 0.5:
            recommendations.append(
                "ğŸ’¡ ãƒ“ã‚¸ãƒã‚¹ç”¨èªãŒå¤šãæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å®Ÿå‹™ã¸ã®å¿œç”¨äº‹ä¾‹ã‚’å«ã‚ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
            )

        # å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºã®ãƒãƒ©ãƒ³ã‚¹ã‚’ç¢ºèª
        introduction_count = phases.get('introduction', 0)
        understanding_count = phases.get('understanding', 0)
        application_count = phases.get('application', 0)

        if introduction_count < len(terms) * 0.2:
            recommendations.append(
                "âš ï¸ å°å…¥ãƒ•ã‚§ãƒ¼ã‚ºã®ç”¨èªãŒå°‘ãªã„ã§ã™ã€‚åŸºæœ¬æ¦‚å¿µã®èª¬æ˜ã‚’å……å®Ÿã•ã›ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
            )

        if application_count < len(terms) * 0.2:
            recommendations.append(
                "âš ï¸ å®Ÿè·µãƒ•ã‚§ãƒ¼ã‚ºã®ç”¨èªãŒå°‘ãªã„ã§ã™ã€‚å…·ä½“çš„ãªæ´»ç”¨æ–¹æ³•ã‚„äº‹ä¾‹ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
            )

        if not recommendations:
            recommendations.append("âœ“ ç”¨èªã®ãƒãƒ©ãƒ³ã‚¹ã¯è‰¯å¥½ã§ã™ã€‚")

        # é‡è¦ç”¨èªã®æ´»ç”¨ææ¡ˆ
        top_5_terms = [term['term'] for term in terms[:5]]
        recommendations.append(
            f"ğŸ’¡ é‡è¦ç”¨èªãƒˆãƒƒãƒ—5: {', '.join(top_5_terms)}\n  ã“ã‚Œã‚‰ã®ç”¨èªã‚’è¬›åº§ã®å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§é©åˆ‡ã«è§£èª¬ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
        )

        return recommendations

    def save_report(self, report: Dict, output_path: str):
        """ç”¨èªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ç”¨èªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {output_path}")
        except Exception as e:
            print(f"âœ— ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)

    def print_report_summary(self, report: Dict):
        """ç”¨èªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("\n" + "=" * 60)
        print("ğŸ“– è¬›åº§ç”¨èªåˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 60)

        summary = report.get('terminology_summary', {})

        print(f"\nğŸ“Š ç”¨èªã‚µãƒãƒªãƒ¼:")
        print(f"  - ãƒ¦ãƒ‹ãƒ¼ã‚¯ç”¨èªæ•°: {summary.get('total_unique_terms', 0)}å€‹")
        print(f"  - ãƒˆãƒƒãƒ—ç”¨èªæ•°: {summary.get('top_terms_count', 0)}å€‹")

        # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ
        categories = summary.get('categories', {})
        if categories:
            print(f"\nğŸ·ï¸ ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ:")
            category_labels = {
                'technical': 'æŠ€è¡“ç”¨èª',
                'business': 'ãƒ“ã‚¸ãƒã‚¹ç”¨èª',
                'learning': 'å­¦ç¿’ç”¨èª',
                'general': 'ä¸€èˆ¬ç”¨èª'
            }
            for cat, count in categories.items():
                label = category_labels.get(cat, cat)
                print(f"  - {label}: {count}å€‹")

        # å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºåˆ†å¸ƒ
        phases = summary.get('learning_phases', {})
        if phases:
            print(f"\nğŸ“š å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºåˆ†å¸ƒ:")
            phase_labels = {
                'introduction': 'å°å…¥',
                'understanding': 'ç†è§£',
                'application': 'å®Ÿè·µ'
            }
            for phase, count in phases.items():
                label = phase_labels.get(phase, phase)
                print(f"  - {label}: {count}å€‹")

        # ãƒˆãƒƒãƒ—10ç”¨èª
        top_terms = report.get('top_terms', [])[:10]
        if top_terms:
            print(f"\nğŸ” é »å‡ºç”¨èªãƒˆãƒƒãƒ—10:")
            for i, term_info in enumerate(top_terms, 1):
                term = term_info['term']
                freq = term_info['frequency']
                category = term_info['category']
                phase = term_info.get('learning_phase', 'unknown')
                print(f"  {i:2d}. {term} (é »åº¦: {freq}, ã‚«ãƒ†ã‚´ãƒª: {category}, ãƒ•ã‚§ãƒ¼ã‚º: {phase})")

        # æ¨å¥¨äº‹é …
        print(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
        for rec in report.get('recommendations', []):
            print(f"  {rec}")

        print("=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description='è¬›åº§ç”¨èªåˆ†æãƒ„ãƒ¼ãƒ« - ãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é‡è¦ç”¨èªã‚’æŠ½å‡ºãƒ»åˆ†æ'
    )

    parser.add_argument(
        '--web-research',
        help='Webãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ï¼ˆJSONï¼‰ã®ãƒ‘ã‚¹'
    )
    parser.add_argument(
        '--youtube-research',
        help='YouTubeæ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ï¼ˆJSONï¼‰ã®ãƒ‘ã‚¹'
    )
    parser.add_argument(
        '--course-theme',
        help='è¬›åº§ãƒ†ãƒ¼ãƒï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰'
    )
    parser.add_argument(
        '--output',
        default='terminology_analysis_report.json',
        help='ç”¨èªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: terminology_analysis_report.jsonï¼‰'
    )

    args = parser.parse_args()

    # å°‘ãªãã¨ã‚‚1ã¤ã®å…¥åŠ›ãŒå¿…è¦
    if not args.web_research and not args.youtube_research:
        parser.error("--web-research ã¾ãŸã¯ --youtube-research ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")

    print("=" * 60)
    print("ğŸ“– è¬›åº§ç”¨èªåˆ†æãƒ„ãƒ¼ãƒ« - å·¥ç¨‹1D")
    print("=" * 60)

    analyzer = CourseTerminologyAnalyzer()

    # Webãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç”¨èªã‚’æŠ½å‡º
    web_terms = {'status': 'skipped'}
    if args.web_research:
        print(f"\nğŸ“š Webãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç”¨èªã‚’æŠ½å‡ºä¸­: {args.web_research}")
        try:
            with open(args.web_research, 'r', encoding='utf-8') as f:
                web_data = json.load(f)
            web_terms = analyzer.extract_terminology_from_web(web_data)
            print(f"  âœ“ {web_terms.get('total_unique_terms', 0)}å€‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ç”¨èªã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
            web_terms = {'status': 'error', 'message': str(e)}

    # YouTubeæ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç”¨èªã‚’æŠ½å‡º
    youtube_terms = {'status': 'skipped'}
    if args.youtube_research:
        print(f"\nğŸ¥ YouTubeæ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç”¨èªã‚’æŠ½å‡ºä¸­: {args.youtube_research}")
        try:
            with open(args.youtube_research, 'r', encoding='utf-8') as f:
                youtube_data = json.load(f)
            youtube_terms = analyzer.extract_terminology_from_youtube(youtube_data)
            print(f"  âœ“ {youtube_terms.get('total_unique_terms', 0)}å€‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ç”¨èªã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
            youtube_terms = {'status': 'error', 'message': str(e)}

    # çµ±åˆç”¨èªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    report = analyzer.generate_terminology_report(
        web_terms,
        youtube_terms,
        args.course_theme
    )

    # ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
    analyzer.save_report(report, args.output)

    # ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    analyzer.print_report_summary(report)


if __name__ == '__main__':
    main()
