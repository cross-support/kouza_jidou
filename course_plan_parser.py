import argparse
import pandas as pd
import json
import sys
from typing import Optional, Dict

def parse_course_plan(csv_path, course_name, unit_number=None):
    """CSVã‹ã‚‰ç‰¹å®šã®è¬›åº§ã®æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹"""
    try:
        df = pd.read_csv(csv_path, header=0, usecols=range(6), dtype=str)
        df.columns = ['category', 'course', 'unit_no', 'unit_name', 'slide_no', 'slide_title']
        df = df.dropna(subset=['course', 'slide_title'])
        df = df[df['course'] != 'è¬›åº§å']
    except (FileNotFoundError, ValueError) as e:
        print(f"Error reading or parsing CSV: {e}", file=sys.stderr)
        sys.exit(1)

    course_df = df[df['course'] == course_name].copy()
    if course_df.empty:
        all_courses = df['course'].unique()
        similar_courses = [c for c in all_courses if course_name.lower() in c.lower()]
        print(f"Error: Course '{course_name}' not found.", file=sys.stderr)
        if similar_courses:
            print(f"Did you mean one of these? {', '.join(similar_courses)}", file=sys.stderr)
        sys.exit(1)

    if unit_number:
        # è¤‡æ•°ãƒ¦ãƒ‹ãƒƒãƒˆå¯¾å¿œ: ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°æŒ‡å®šå¯èƒ½ (ä¾‹: "1,2,3,4")
        unit_numbers = [u.strip() for u in str(unit_number).split(',')]
        course_df = course_df[course_df['unit_no'].isin(unit_numbers)]
        if course_df.empty:
            print(f"Error: Units '{unit_number}' not found for course '{course_name}'.", file=sys.stderr)
            available_units = df[df['course'] == course_name]['unit_no'].unique()
            print(f"Available units: {', '.join(map(str, sorted(available_units)))}", file=sys.stderr)
            sys.exit(1)
    
    course_df['unit_no'] = pd.to_numeric(course_df['unit_no'])
    course_df['slide_no'] = pd.to_numeric(course_df['slide_no'])
    course_df = course_df.sort_values(by=['unit_no', 'slide_no'])

    return course_df

def load_research_data(json_path: str) -> Optional[Dict]:
    """ãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"Warning: Research file not found: {json_path}", file=sys.stderr)
        return None
    except json.JSONDecodeError as e:
        print(f"Warning: Invalid JSON in research file: {json_path} - {e}", file=sys.stderr)
        return None

def format_web_research(research_data: Dict) -> str:
    """Webãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if not research_data or not research_data.get('sources'):
        return ""

    output = ["### ğŸ“š Web ãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿"]
    output.append(f"- åé›†æ—¥: {research_data.get('research_date', 'N/A')}")
    output.append(f"- æƒ…å ±æºæ•°: {research_data.get('total_sources', 0)}ä»¶")
    output.append(f"- ç·æ–‡å­—æ•°: {research_data.get('summary', {}).get('total_characters', 0):,}æ–‡å­—\n")

    for i, source in enumerate(research_data.get('sources', [])[:5], 1):  # æœ€å¤§5ä»¶ã¾ã§è¡¨ç¤º
        output.append(f"**æƒ…å ±æº{i}: {source.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')}**")
        output.append(f"- URL: {source.get('url', 'N/A')}")
        output.append(f"- æ–‡å­—æ•°: {source.get('character_count', 0):,}æ–‡å­—")

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è¦ç´„ï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰
        content = source.get('content', '')
        if content:
            preview = content[:500] + "..." if len(content) > 500 else content
            output.append(f"- å†…å®¹æŠœç²‹: {preview}\n")

    if len(research_data.get('sources', [])) > 5:
        remaining = len(research_data.get('sources', [])) - 5
        output.append(f"ï¼ˆä»–{remaining}ä»¶ã®æƒ…å ±æºã‚’çœç•¥ï¼‰\n")

    return "\n".join(output)

def format_youtube_research(research_data: Dict) -> str:
    """YouTubeæ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if not research_data or not research_data.get('transcriptions'):
        return ""

    output = ["### ğŸ¥ YouTube æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿"]
    output.append(f"- æ–‡å­—èµ·ã“ã—æ—¥: {research_data.get('transcription_date', 'N/A')}")
    output.append(f"- å‹•ç”»æ•°: {research_data.get('successful_transcriptions', 0)}ä»¶")
    output.append(f"- ç·æ–‡å­—æ•°: {research_data.get('summary', {}).get('total_words', 0):,}æ–‡å­—")
    output.append(f"- ç·å†ç”Ÿæ™‚é–“: {research_data.get('summary', {}).get('total_duration', 0)/60:.1f}åˆ†\n")

    for i, transcript in enumerate(research_data.get('transcriptions', [])[:3], 1):  # æœ€å¤§3ä»¶ã¾ã§è¡¨ç¤º
        output.append(f"**å‹•ç”»{i}: {transcript.get('video_id', 'N/A')}**")
        output.append(f"- URL: {transcript.get('source_url', 'N/A')}")
        output.append(f"- è¨€èª: {transcript.get('language', 'N/A')}")
        output.append(f"- æ–‡å­—æ•°: {transcript.get('word_count', 0):,}æ–‡å­—")
        output.append(f"- å‹•ç”»æ™‚é–“: {transcript.get('total_duration', 0)/60:.1f}åˆ†")

        # æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã®è¦ç´„ï¼ˆæœ€åˆã®800æ–‡å­—ï¼‰
        text = transcript.get('text', '')
        if text:
            preview = text[:800] + "..." if len(text) > 800 else text
            output.append(f"- å†…å®¹æŠœç²‹: {preview}\n")

    if len(research_data.get('transcriptions', [])) > 3:
        remaining = len(research_data.get('transcriptions', [])) - 3
        output.append(f"ï¼ˆä»–{remaining}ä»¶ã®å‹•ç”»ã‚’çœç•¥ï¼‰\n")

    return "\n".join(output)

def format_quality_assurance(quality_report: Dict, terminology_report: Dict) -> str:
    """å“è³ªä¿è¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if not quality_report and not terminology_report:
        return ""

    output = ["### ğŸ” å“è³ªä¿è¨¼ãƒ‡ãƒ¼ã‚¿ï¼ˆè¬›åº§ä½œæˆã®æŒ‡é‡ï¼‰"]
    output.append("ä»¥ä¸‹ã®å“è³ªåˆ†æçµæœã‚’å‚è€ƒã«ã€æ­£ç¢ºã§æ•™è‚²çš„ä¾¡å€¤ã®é«˜ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n")

    # å“è³ªæ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ
    if quality_report:
        overall_quality = quality_report.get('overall_quality', 'unknown')
        quality_labels = {
            'excellent': 'å„ªç§€',
            'good': 'è‰¯å¥½',
            'acceptable': 'è¨±å®¹ç¯„å›²',
            'needs_improvement': 'æ”¹å–„å¿…è¦'
        }
        quality_label = quality_labels.get(overall_quality, 'ä¸æ˜')

        output.append(f"**å“è³ªè©•ä¾¡**: {quality_label}")

        summary = quality_report.get('integrated_summary', {})
        output.append(f"- ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°: {summary.get('total_data_points', 0)}ä»¶")
        output.append(f"- ä¿¡é ¼æ€§ã®é«˜ã„æƒ…å ±æº: {summary.get('credible_sources', 0)}ä»¶")

        # æ¨å¥¨äº‹é …ï¼ˆæœ€åˆã®3ä»¶ï¼‰
        recommendations = quality_report.get('quality_recommendations', [])[:3]
        if recommendations:
            output.append("\n**å“è³ªã«é–¢ã™ã‚‹æ³¨æ„ç‚¹**:")
            for rec in recommendations:
                output.append(f"- {rec}")

    # ç”¨èªåˆ†æãƒ¬ãƒãƒ¼ãƒˆ
    if terminology_report:
        summary = terminology_report.get('terminology_summary', {})
        output.append(f"\n**é‡è¦ç”¨èªåˆ†æ**:")
        output.append(f"- æ¤œå‡ºã•ã‚ŒãŸé‡è¦ç”¨èªæ•°: {summary.get('total_unique_terms', 0)}å€‹")

        # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ
        categories = summary.get('categories', {})
        if categories:
            cat_str = ', '.join([f"{k}: {v}å€‹" for k, v in categories.items()])
            output.append(f"- ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ: {cat_str}")

        # å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºåˆ†å¸ƒ
        phases = summary.get('learning_phases', {})
        if phases:
            phase_labels = {
                'introduction': 'å°å…¥',
                'understanding': 'ç†è§£',
                'application': 'å®Ÿè·µ'
            }
            phase_str = ', '.join([f"{phase_labels.get(k, k)}: {v}å€‹" for k, v in phases.items()])
            output.append(f"- å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºåˆ†å¸ƒ: {phase_str}")

        # ãƒˆãƒƒãƒ—10ç”¨èª
        top_terms = terminology_report.get('top_terms', [])[:10]
        if top_terms:
            terms_list = [term['term'] for term in top_terms]
            output.append(f"\n**å¿…ãšè§£èª¬ã™ã¹ãé‡è¦ç”¨èªãƒˆãƒƒãƒ—10**:")
            output.append(f"{', '.join(terms_list)}")
            output.append("\nâ†’ ã“ã‚Œã‚‰ã®ç”¨èªã¯è¬›åº§å†…ã§æ˜ç¢ºã«å®šç¾©ã—ã€é©åˆ‡ã«è§£èª¬ã—ã¦ãã ã•ã„ã€‚")

        # æ¨å¥¨äº‹é …ï¼ˆæœ€åˆã®2ä»¶ï¼‰
        recommendations = terminology_report.get('recommendations', [])[:2]
        if recommendations:
            output.append("\n**ç”¨èªã«é–¢ã™ã‚‹æ¨å¥¨äº‹é …**:")
            for rec in recommendations:
                output.append(f"- {rec}")

    output.append("")  # ç©ºè¡Œ
    return "\n".join(output)

def format_as_prompt(df, course_name):
    """LLMã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼ã§è¬›åº§æ§‹æˆã‚’å‡ºåŠ›"""
    if df.empty:
        return f"# {course_name}\n\nï¼ˆã“ã®è¬›åº§ã«ã¯ã‚¹ãƒ©ã‚¤ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼‰"
    output = [f"## è¬›åº§å: {course_name}\n"]
    for unit_no, group in df.groupby('unit_no'):
        unit_name = group['unit_name'].iloc[0]
        output.append(f"### ãƒ¦ãƒ‹ãƒƒãƒˆ{int(unit_no)}: {unit_name}")
        for record in group.to_dict('records'):
            output.append(f"- ã‚¹ãƒ©ã‚¤ãƒ‰{record['slide_no']}: {record['slide_title']}")
    return "\n".join(output)

def format_as_canvas_and_narration_prompt(df, args, web_research_data=None, youtube_research_data=None, quality_report=None, terminology_report=None):
    """Geminiã«Canvasç”¨è¨­è¨ˆå›³ã¨ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å°æœ¬ã‚’ä¸€åº¦ã«ç”Ÿæˆã•ã›ã‚‹ãŸã‚ã®æŒ‡ç¤ºæ›¸ã‚’ç”Ÿæˆ"""
    structure_prompt = format_as_prompt(df, args.course)

    # ãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    research_section = ""
    if web_research_data or youtube_research_data:
        research_parts = []
        research_parts.append("# ğŸ“Š äº‹å‰ãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ï¼ˆè¬›åº§ä½œæˆã®å‚è€ƒæƒ…å ±ï¼‰")
        research_parts.append("ä»¥ä¸‹ã®ãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã¯ã€è¬›åº§å†…å®¹ã‚’ã‚ˆã‚Šæ­£ç¢ºã§å®Ÿè·µçš„ãªã‚‚ã®ã«ã™ã‚‹ãŸã‚ã®å‚è€ƒè³‡æ–™ã§ã™ã€‚")
        research_parts.append("ã“ã‚Œã‚‰ã®æƒ…å ±ã‚’æ´»ç”¨ã—ã¦ã€æœ€æ–°ã‹ã¤ä¿¡é ¼æ€§ã®é«˜ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n")

        if web_research_data:
            web_formatted = format_web_research(web_research_data)
            if web_formatted:
                research_parts.append(web_formatted)

        if youtube_research_data:
            youtube_formatted = format_youtube_research(youtube_research_data)
            if youtube_formatted:
                research_parts.append(youtube_formatted)

        # å“è³ªä¿è¨¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        qa_formatted = format_quality_assurance(quality_report, terminology_report)
        if qa_formatted:
            research_parts.append(qa_formatted)

        research_parts.append("---\n")
        research_section = "\n".join(research_parts)

    canvas_prompt = f"""
ã‚ãªãŸã¯ã€eãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°è¬›åº§ã®ã€Œã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒŠãƒ«ãƒ‡ã‚¶ã‚¤ãƒŠãƒ¼ã€ã€Œãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ‡ã‚¶ã‚¤ãƒŠãƒ¼ã€ã€ŒãƒŠãƒ¬ãƒ¼ã‚¿ãƒ¼ã€ã‚’å…¼å‹™ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
ã“ã‚Œã‹ã‚‰ã€è¬›åº§ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã€æŒ‡å®šã•ã‚ŒãŸ2ã¤ã®ãƒ‘ãƒ¼ãƒˆã«åˆ†ã‘ã¦ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

# è¬›åº§ã®å…¨ä½“ä»•æ§˜
- **è¬›åº§ãƒ†ãƒ¼ãƒ**: {args.course}
- **å—è¬›è€…åƒ**: {args.learner_profile}
- **åˆ°é”ç›®æ¨™ï¼ˆã‚´ãƒ¼ãƒ«è¡Œå‹•ï¼‰**: {args.target_behavior}
- **æƒ³å®šæ™‚é–“**: {args.duration}
- **ãƒˆãƒ¼ãƒ³ï¼†ãƒãƒŠãƒ¼**: {args.tone}

# ç”Ÿæˆå¯¾è±¡ã®è¬›åº§æ§‹æˆï¼ˆã“ã®æ§‹æˆã‚’å³å®ˆã—ã¦ãã ã•ã„ï¼‰
---
{structure_prompt}
---

{research_section}# ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯
ä»¥ä¸‹ã®ã€Œãƒ‘ãƒ¼ãƒˆ1ã€ã¨ã€Œãƒ‘ãƒ¼ãƒˆ2ã€ã‚’ã€ã“ã®é †ç•ªã§ã€ä¸¡æ–¹ã¨ã‚‚ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

---
## ãƒ‘ãƒ¼ãƒˆ1ï¼šGemini Canvasç”¨ãƒ»ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¹ãƒ©ã‚¤ãƒ‰è¨­è¨ˆå›³

ä¸‹è¨˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¾“ã„ã€å…¨ã‚¹ãƒ©ã‚¤ãƒ‰ã®è¦–è¦šçš„ãªè¨­è¨ˆå›³ã‚’Markdownã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã¯ã€ãƒ‡ã‚¶ã‚¤ãƒŠãƒ¼ã‚„Geminiã®Canvasãƒ¢ãƒ¼ãƒ‰ãŒã‚¹ãƒ©ã‚¤ãƒ‰ã‚’è¦–è¦šçš„ã«ä½œæˆã™ã‚‹ãŸã‚ã®æŒ‡ç¤ºæ›¸ã¨ãªã‚Šã¾ã™ã€‚ãƒ¢ãƒ€ãƒ³ã§åˆ†ã‹ã‚Šã‚„ã™ã„ãƒ‡ã‚¶ã‚¤ãƒ³ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚

### ãƒ¦ãƒ‹ãƒƒãƒˆ [ãƒ¦ãƒ‹ãƒƒãƒˆç•ªå·]: [ãƒ¦ãƒ‹ãƒƒãƒˆå]

**ã‚¹ãƒ©ã‚¤ãƒ‰ [ã‚¹ãƒ©ã‚¤ãƒ‰ç•ªå·]: [ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«]**
- **ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ**: ã‚¹ãƒ©ã‚¤ãƒ‰å…¨ä½“ã®æ§‹æˆã‚’æŒ‡ç¤ºã—ã¾ã™ã€‚ï¼ˆä¾‹ï¼šã€Œã‚¿ã‚¤ãƒˆãƒ«ã‚’ä¸Šéƒ¨ã«é…ç½®ã—ã€ä¸­å¤®ã«å¤§ããªã‚¢ã‚¤ã‚³ãƒ³ã‚’é…ç½®ã™ã‚‹ã€ã€Œå·¦ã«ç”»åƒã€å³ã«3ã¤ã®ç®‡æ¡æ›¸ããƒ†ã‚­ã‚¹ãƒˆã€ï¼‰
- **ã‚­ãƒ¼ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«**: ä¸­å¿ƒã¨ãªã‚‹ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯è¦ç´ ã‚’å…·ä½“çš„ã«æŒ‡ç¤ºã—ã¾ã™ã€‚ï¼ˆä¾‹ï¼šã€Œã‚·ãƒ³ãƒ—ãƒ«ãªé›»çƒã®ã‚¢ã‚¤ã‚³ãƒ³ã€ã€Œãƒ‡ãƒ¼ã‚¿ã‚’ä¿è­·ã™ã‚‹ã‚¤ãƒ¡ãƒ¼ã‚¸ã®æŠ½è±¡çš„ãªã‚¤ãƒ©ã‚¹ãƒˆã€ã€ŒChatGPTã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›ç”»é¢ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã€ï¼‰
- **ã‚¹ãƒ©ã‚¤ãƒ‰å†…ãƒ†ã‚­ã‚¹ãƒˆ**: ã‚¹ãƒ©ã‚¤ãƒ‰ã«è¡¨ç¤ºã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£ç¢ºã«è¨˜è¿°ã—ã¾ã™ã€‚ã‚¿ã‚¤ãƒˆãƒ«ä»¥å¤–ã¯ã€3ã€œ5è¡Œã®çŸ­ã„ç®‡æ¡æ›¸ãã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ç•™ã‚ã€ã‚·ãƒ³ãƒ—ãƒ«ã«ã—ã¦ãã ã•ã„ã€‚
- **æ¨å¥¨ã‚«ãƒ©ãƒ¼**: ã‚¹ãƒ©ã‚¤ãƒ‰ã®åŸºèª¿ã¨ãªã‚‹è‰²ã‚’2ã€œ3è‰²ææ¡ˆã—ã¾ã™ã€‚ï¼ˆä¾‹ï¼šã€ŒåŸºèª¿ã¯è½ã¡ç€ã„ãŸé’(#3366CC)ã€å¼·èª¿è‰²ã«ã‚ªãƒ¬ãƒ³ã‚¸(#FF8C00)ã€ï¼‰

---
## ãƒ‘ãƒ¼ãƒˆ2ï¼šã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»å­—å¹•å°æœ¬

è¬›åº§ã®ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã€å‹•ç”»ç”¨ã®å­—å¹•ã‚’ç”Ÿæˆã—ã¾ã™ã€‚ä¸‹è¨˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¾“ã„ã€Markdownã®ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

### å°æœ¬ä½œæˆã®é‡è¦ãƒ«ãƒ¼ãƒ«
1.  **æ™‚é–“è¨ˆç®—**: ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®é€Ÿåº¦ã‚’ã€Œåˆ†é€Ÿ150ãƒ¯ãƒ¼ãƒ‰ï¼ˆ1ç§’ã‚ãŸã‚Š2.5ãƒ¯ãƒ¼ãƒ‰ï¼‰ã€ã¨ä»®å®šã—ã€ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å˜èªæ•°ã‹ã‚‰å„ãƒ–ãƒ­ãƒƒã‚¯ã®ã€Œé–‹å§‹æ™‚é–“ã€ã¨ã€Œçµ‚äº†æ™‚é–“ã€ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚
2.  **å­—å¹•ã®åˆ†å‰²**: ã€ŒãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å…¨æ–‡ã€ã‚’ã€æ„å‘³ãŒé€šã˜ã‚‹å˜ä½ã§çŸ­ã„ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²ã—ã€ã€Œå­—å¹•ãƒ†ã‚­ã‚¹ãƒˆã€ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚å­—å¹•ã¯**å¿…ãš2è¡Œä»¥å†…**ã«åã‚ã¦ãã ã•ã„ã€‚
3.  **ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å½¢å¼**: æ™‚é–“ã¯ `MM:SS` å½¢å¼ï¼ˆä¾‹: `00:08`, `02:15`ï¼‰ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
4.  **æƒ…å ±ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®å¼·èª¿**: ã“ã®è¬›åº§ã§ã¯ã€å€‹äººæƒ…å ±ã‚„ç¤¾å†…ã®æ©Ÿå¯†æƒ…å ±ã‚’ChatGPTã«å…¥åŠ›ã—ãªã„ã“ã¨ã®é‡è¦æ€§ã‚’ã€æ™‚é–“ã‚’å‰²ã„ã¦æ˜ç¢ºã‹ã¤å¼·åŠ›ã«ä¼ãˆã¦ãã ã•ã„ã€‚ãã®ãŸã‚ã®ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚

### å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆMarkdownãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰

| ã‚¹ãƒ©ã‚¤ãƒ‰ç•ªå· | é–‹å§‹æ™‚é–“ | çµ‚äº†æ™‚é–“ | å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæœ€å¤§2è¡Œï¼‰ | ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å…¨æ–‡ï¼ˆå£èªä½“ï¼‰ |
|---|---|---|---|---|---|n| 1 | 00:00 | 00:05 | ã“ã‚“ã«ã¡ã¯ï¼æœ¬æ—¥ã¯ã€ŒChatGPTæ¥­å‹™æ´»ç”¨ã®åŸºæœ¬ã€<br>ã«ã¤ã„ã¦å­¦ã‚“ã§ã„ãã¾ã—ã‚‡ã†ã€‚ | ã“ã‚“ã«ã¡ã¯ï¼æœ¬æ—¥ã¯ã€ŒChatGPTæ¥­å‹™æ´»ç”¨ã®åŸºæœ¬ã€ã«ã¤ã„ã¦ã€çš†ã•ã‚“ã¨ä¸€ç·’ã«å­¦ã‚“ã§ã„ãã¾ã—ã‚‡ã†ã€‚ |
| 1 | 00:06 | 00:10 | ã“ã®è¬›åº§ã‚’çµ‚ãˆã‚‹é ƒã«ã¯ã€<br>æ˜æ—¥ã‹ã‚‰ä½¿ãˆã‚‹ã‚¹ã‚­ãƒ«ãŒèº«ã«ã¤ã„ã¦ã„ã¾ã™ã€‚| ã“ã®è¬›åº§ã‚’çµ‚ãˆã‚‹é ƒã«ã¯ã€çš†ã•ã‚“ã®æ¥­å‹™ãŒã‚‚ã£ã¨åŠ¹ç‡çš„ã«ãªã‚‹ã€æ˜æ—¥ã‹ã‚‰ã™ãã«ä½¿ãˆã‚‹ã‚¹ã‚­ãƒ«ãŒèº«ã«ã¤ã„ã¦ã„ã¾ã™ã‚ˆã€‚ |
| ... | ... | ... | ... | ... |

"""
    return canvas_prompt

def main():
    parser = argparse.ArgumentParser(description="Parse course plan CSV and generate structured output for course creation.")
    parser.add_argument("--csv", required=True, help="Path to the course plan CSV file.")
    parser.add_argument("--course", required=True, help="The name of the course to extract.")
    parser.add_argument("--unit", help="(Optional) Specific unit number to filter by.")
    parser.add_argument(
        "--format",
        required=True,
        choices=["canvas-script"], # Kept as a single choice for now
        help="Output format. 'canvas-script' generates a full prompt for Gemini to create slide designs and a timed narration script.",
    )
    # Arguments for the prompt
    parser.add_argument("--learner_profile", required=True, help="Who the learners are.")
    parser.add_argument("--target_behavior", required=True, help="What learners should be able to do.")
    parser.add_argument("--duration", required=True, help="Estimated duration of the course.")
    parser.add_argument("--tone", required=True, help="Tone and manner of the course.")

    # å·¥ç¨‹1A/1B ãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    parser.add_argument("--web-research", help="(Optional) Path to web research JSON file (from course_research.py).")
    parser.add_argument("--youtube-research", help="(Optional) Path to YouTube transcription JSON file (from youtube_transcriber.py).")

    # å·¥ç¨‹1C/1D å“è³ªä¿è¨¼ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    parser.add_argument("--quality-report", help="(Optional) Path to quality validation report JSON file (from course_quality_validator.py).")
    parser.add_argument("--terminology-report", help="(Optional) Path to terminology analysis report JSON file (from course_terminology_analyzer.py).")

    args = parser.parse_args()

    df = parse_course_plan(args.csv, args.course, args.unit)

    # ãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    web_research_data = None
    youtube_research_data = None

    if args.web_research:
        print(f"ğŸ“š Webãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {args.web_research}", file=sys.stderr)
        web_research_data = load_research_data(args.web_research)
        if web_research_data:
            print(f"  âœ“ {web_research_data.get('total_sources', 0)}ä»¶ã®æƒ…å ±æºã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ", file=sys.stderr)

    if args.youtube_research:
        print(f"ğŸ¥ YouTubeæ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {args.youtube_research}", file=sys.stderr)
        youtube_research_data = load_research_data(args.youtube_research)
        if youtube_research_data:
            print(f"  âœ“ {youtube_research_data.get('successful_transcriptions', 0)}ä»¶ã®å‹•ç”»ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ", file=sys.stderr)

    # å“è³ªä¿è¨¼ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    quality_report = None
    terminology_report = None

    if args.quality_report:
        print(f"ğŸ” å“è³ªæ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­: {args.quality_report}", file=sys.stderr)
        quality_report = load_research_data(args.quality_report)
        if quality_report:
            quality = quality_report.get('overall_quality', 'unknown')
            print(f"  âœ“ å“è³ªè©•ä¾¡: {quality}", file=sys.stderr)

    if args.terminology_report:
        print(f"ğŸ“– ç”¨èªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­: {args.terminology_report}", file=sys.stderr)
        terminology_report = load_research_data(args.terminology_report)
        if terminology_report:
            term_count = terminology_report.get('terminology_summary', {}).get('total_unique_terms', 0)
            print(f"  âœ“ {term_count}å€‹ã®é‡è¦ç”¨èªã‚’æ¤œå‡º", file=sys.stderr)

    if args.format == 'canvas-script':
        output = format_as_canvas_and_narration_prompt(df, args, web_research_data, youtube_research_data, quality_report, terminology_report)
    else:
        # Fallback, though argparse should prevent this.
        parser.error(f"Invalid format specified.")

    print(output)

if __name__ == "__main__":
    main()