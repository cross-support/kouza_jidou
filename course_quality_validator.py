#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¬›åº§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªæ¤œè¨¼ãƒ„ãƒ¼ãƒ« - å·¥ç¨‹1C

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚’æ¤œè¨¼ã—ã€
è¬›åº§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆã®ãŸã‚ã®å“è³ªä¿è¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

SEO_AI_ver6.5ã®å·¥ç¨‹8ï¼ˆãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ï¼‰ã‚’è¬›åº§è‡ªå‹•åŒ–ã«é©å¿œã€‚
"""

import argparse
import json
import re
import sys
from datetime import datetime
from typing import Dict, List, Optional
from urllib.parse import urlparse


class CourseQualityValidator:
    """è¬›åº§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªæ¤œè¨¼ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.validation_date = datetime.now().isoformat()

    def validate_web_research(self, web_data: Dict) -> Dict:
        """
        Webãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼

        Args:
            web_data: Webãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿

        Returns:
            æ¤œè¨¼çµæœã®è¾æ›¸
        """
        if not web_data or not web_data.get('sources'):
            return {
                'status': 'no_data',
                'message': 'Webãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“'
            }

        sources = web_data.get('sources', [])
        validations = []

        for i, source in enumerate(sources, 1):
            validation = {
                'source_number': i,
                'url': source.get('url', ''),
                'title': source.get('title', ''),
                'checks': {}
            }

            # URLã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
            url = source.get('url', '')
            validation['checks']['url_valid'] = self._validate_url(url)

            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å­˜åœ¨ç¢ºèª
            content = source.get('content', '')
            char_count = source.get('character_count', 0)
            validation['checks']['has_content'] = len(content) > 100 or char_count > 100
            validation['checks']['content_length'] = len(content)

            # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®æ¤œå‡º
            numbers_found = re.findall(r'\d+(?:\.\d+)?%|\d+(?:,\d{3})*(?:\.\d+)?', content)
            validation['checks']['data_points_found'] = len(numbers_found)
            validation['checks']['sample_data'] = numbers_found[:5] if numbers_found else []

            # ä¿¡é ¼æ€§è©•ä¾¡ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¤å®šï¼‰
            validation['checks']['source_credibility'] = self._evaluate_credibility(url)

            validations.append(validation)

        # ã‚µãƒãƒªãƒ¼
        summary = {
            'total_sources': len(sources),
            'valid_urls': sum(1 for v in validations if v['checks']['url_valid']),
            'sources_with_content': sum(1 for v in validations if v['checks']['has_content']),
            'total_data_points': sum(v['checks']['data_points_found'] for v in validations),
            'credible_sources': sum(1 for v in validations if v['checks']['source_credibility'] in ['high', 'medium'])
        }

        return {
            'status': 'validated',
            'summary': summary,
            'validations': validations,
            'recommendations': self._generate_web_recommendations(summary, validations)
        }

    def validate_youtube_research(self, youtube_data: Dict) -> Dict:
        """
        YouTubeæ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼

        Args:
            youtube_data: YouTubeæ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿

        Returns:
            æ¤œè¨¼çµæœã®è¾æ›¸
        """
        if not youtube_data or not youtube_data.get('transcriptions'):
            return {
                'status': 'no_data',
                'message': 'YouTubeæ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“'
            }

        transcriptions = youtube_data.get('transcriptions', [])
        validations = []

        for i, transcript in enumerate(transcriptions, 1):
            validation = {
                'video_number': i,
                'video_id': transcript.get('video_id', ''),
                'url': transcript.get('source_url', ''),
                'checks': {}
            }

            # æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã®å“è³ªãƒã‚§ãƒƒã‚¯
            text = transcript.get('text', '')
            word_count = transcript.get('word_count', 0)

            validation['checks']['has_transcript'] = len(text) > 100 or word_count > 100
            validation['checks']['word_count'] = word_count
            validation['checks']['duration_minutes'] = transcript.get('total_duration', 0) / 60

            # è¨€èªç¢ºèª
            validation['checks']['language'] = transcript.get('language', 'unknown')

            # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®æ¤œå‡º
            numbers_found = re.findall(r'\d+(?:\.\d+)?%|\d+(?:,\d{3})*(?:\.\d+)?', text)
            validation['checks']['data_points_found'] = len(numbers_found)
            validation['checks']['sample_data'] = numbers_found[:5] if numbers_found else []

            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°
            segments = transcript.get('segments', [])
            validation['checks']['segment_count'] = len(segments)

            validations.append(validation)

        # ã‚µãƒãƒªãƒ¼
        summary = {
            'total_videos': len(transcriptions),
            'videos_with_transcripts': sum(1 for v in validations if v['checks']['has_transcript']),
            'total_words': sum(v['checks']['word_count'] for v in validations),
            'total_duration_minutes': sum(v['checks']['duration_minutes'] for v in validations),
            'total_data_points': sum(v['checks']['data_points_found'] for v in validations),
            'languages': list(set(v['checks']['language'] for v in validations))
        }

        return {
            'status': 'validated',
            'summary': summary,
            'validations': validations,
            'recommendations': self._generate_youtube_recommendations(summary, validations)
        }

    def generate_quality_report(self, web_validation: Dict, youtube_validation: Dict) -> Dict:
        """
        çµ±åˆå“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ

        Args:
            web_validation: Webæ¤œè¨¼çµæœ
            youtube_validation: YouTubeæ¤œè¨¼çµæœ

        Returns:
            çµ±åˆå“è³ªãƒ¬ãƒãƒ¼ãƒˆ
        """
        report = {
            'validation_date': self.validation_date,
            'overall_quality': 'unknown',
            'web_research': web_validation,
            'youtube_research': youtube_validation,
            'integrated_summary': {},
            'quality_recommendations': []
        }

        # çµ±åˆã‚µãƒãƒªãƒ¼
        web_summary = web_validation.get('summary', {})
        youtube_summary = youtube_validation.get('summary', {})

        report['integrated_summary'] = {
            'total_information_sources': (
                web_summary.get('total_sources', 0) +
                youtube_summary.get('total_videos', 0)
            ),
            'total_data_points': (
                web_summary.get('total_data_points', 0) +
                youtube_summary.get('total_data_points', 0)
            ),
            'credible_sources': web_summary.get('credible_sources', 0),
            'total_content_volume': {
                'web_characters': sum(
                    v['checks']['content_length']
                    for v in web_validation.get('validations', [])
                ),
                'youtube_words': youtube_summary.get('total_words', 0)
            }
        }

        # ç·åˆå“è³ªè©•ä¾¡
        report['overall_quality'] = self._evaluate_overall_quality(report['integrated_summary'])

        # çµ±åˆæ¨å¥¨äº‹é …
        report['quality_recommendations'] = self._generate_integrated_recommendations(
            web_validation,
            youtube_validation,
            report['integrated_summary']
        )

        return report

    def _validate_url(self, url: str) -> bool:
        """URLã®åŸºæœ¬çš„ãªæœ‰åŠ¹æ€§ã‚’ç¢ºèª"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except Exception:
            return False

    def _evaluate_credibility(self, url: str) -> str:
        """æƒ…å ±æºã®ä¿¡é ¼æ€§ã‚’è©•ä¾¡"""
        if not url:
            return 'unknown'

        # é«˜ä¿¡é ¼æ€§ãƒ‰ãƒ¡ã‚¤ãƒ³
        high_credibility = [
            'wikipedia.org', '.gov', '.edu', '.go.jp', '.ac.jp',
            'scholar.google', 'researchgate.net', 'arxiv.org'
        ]

        # ä¸­ç¨‹åº¦ã®ä¿¡é ¼æ€§
        medium_credibility = [
            'itmedia.co.jp', 'nikkei.com', 'diamond.jp', 'forbes.com',
            'techcrunch.com', 'qiita.com', 'zenn.dev', 'github.com'
        ]

        url_lower = url.lower()

        for domain in high_credibility:
            if domain in url_lower:
                return 'high'

        for domain in medium_credibility:
            if domain in url_lower:
                return 'medium'

        return 'low'

    def _generate_web_recommendations(self, summary: Dict, validations: List[Dict]) -> List[str]:
        """Webæ¤œè¨¼çµæœã‹ã‚‰æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒä¸è¶³ã—ã¦ã„ã‚‹æƒ…å ±æº
        if summary['sources_with_content'] < summary['total_sources']:
            missing = summary['total_sources'] - summary['sources_with_content']
            recommendations.append(
                f"âš ï¸ {missing}ä»¶ã®æƒ…å ±æºã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚åˆ¥ã®URLã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )

        # ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆãŒå°‘ãªã„
        if summary['total_data_points'] < 10:
            recommendations.append(
                f"âš ï¸ æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ï¼ˆ{summary['total_data_points']}ä»¶ï¼‰ã§ã™ã€‚çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€æƒ…å ±æºã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
            )

        # ä¿¡é ¼æ€§ã®ä½ã„æƒ…å ±æºãŒå¤šã„
        credible_ratio = summary['credible_sources'] / summary['total_sources'] if summary['total_sources'] > 0 else 0
        if credible_ratio < 0.5:
            recommendations.append(
                f"âš ï¸ ä¿¡é ¼æ€§ã®é«˜ã„æƒ…å ±æºãŒå°‘ãªã„ï¼ˆ{summary['credible_sources']}/{summary['total_sources']}ä»¶ï¼‰ã§ã™ã€‚"
                "å…¬çš„æ©Ÿé–¢ã‚„å­¦è¡“æ©Ÿé–¢ã®æƒ…å ±æºã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
            )

        if not recommendations:
            recommendations.append("âœ“ Webãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã¯è‰¯å¥½ãªå“è³ªã§ã™ã€‚")

        return recommendations

    def _generate_youtube_recommendations(self, summary: Dict, validations: List[Dict]) -> List[str]:
        """YouTubeæ¤œè¨¼çµæœã‹ã‚‰æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []

        # æ–‡å­—èµ·ã“ã—ãŒä¸è¶³
        if summary['videos_with_transcripts'] < summary['total_videos']:
            missing = summary['total_videos'] - summary['videos_with_transcripts']
            recommendations.append(
                f"âš ï¸ {missing}ä»¶ã®å‹•ç”»ã§æ–‡å­—èµ·ã“ã—ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚å­—å¹•ã®ã‚ã‚‹å‹•ç”»ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚"
            )

        # ç·æ–‡å­—æ•°ãŒå°‘ãªã„
        if summary['total_words'] < 5000:
            recommendations.append(
                f"âš ï¸ ç·æ–‡å­—æ•°ãŒå°‘ãªã„ï¼ˆ{summary['total_words']:,}èªï¼‰ã§ã™ã€‚ã‚ˆã‚Šé•·ã„å‹•ç”»ã¾ãŸã¯è¿½åŠ ã®å‹•ç”»ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )

        # å‹•ç”»æ™‚é–“ãŒçŸ­ã„
        if summary['total_duration_minutes'] < 10:
            recommendations.append(
                f"âš ï¸ ç·å‹•ç”»æ™‚é–“ãŒçŸ­ã„ï¼ˆ{summary['total_duration_minutes']:.1f}åˆ†ï¼‰ã§ã™ã€‚ã‚ˆã‚Šè©³ç´°ãªè§£èª¬å‹•ç”»ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
            )

        if not recommendations:
            recommendations.append("âœ“ YouTubeæ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã¯è‰¯å¥½ãªå“è³ªã§ã™ã€‚")

        return recommendations

    def _evaluate_overall_quality(self, integrated_summary: Dict) -> str:
        """çµ±åˆã‚µãƒãƒªãƒ¼ã‹ã‚‰ç·åˆå“è³ªã‚’è©•ä¾¡"""
        score = 0

        # æƒ…å ±æºã®æ•°
        total_sources = integrated_summary.get('total_information_sources', 0)
        if total_sources >= 5:
            score += 2
        elif total_sources >= 3:
            score += 1

        # ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®æ•°
        total_data_points = integrated_summary.get('total_data_points', 0)
        if total_data_points >= 20:
            score += 2
        elif total_data_points >= 10:
            score += 1

        # ä¿¡é ¼æ€§ã®é«˜ã„æƒ…å ±æº
        credible_sources = integrated_summary.get('credible_sources', 0)
        if credible_sources >= 3:
            score += 2
        elif credible_sources >= 1:
            score += 1

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒœãƒªãƒ¥ãƒ¼ãƒ 
        youtube_words = integrated_summary.get('total_content_volume', {}).get('youtube_words', 0)
        if youtube_words >= 10000:
            score += 2
        elif youtube_words >= 5000:
            score += 1

        # è©•ä¾¡
        if score >= 7:
            return 'excellent'
        elif score >= 5:
            return 'good'
        elif score >= 3:
            return 'acceptable'
        else:
            return 'needs_improvement'

    def _generate_integrated_recommendations(
        self,
        web_validation: Dict,
        youtube_validation: Dict,
        integrated_summary: Dict
    ) -> List[str]:
        """çµ±åˆæ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []

        quality = self._evaluate_overall_quality(integrated_summary)

        if quality == 'excellent':
            recommendations.append("âœ“ å„ªã‚ŒãŸå“è³ªã®ãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚è¬›åº§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆã«ååˆ†ãªæƒ…å ±ãŒã‚ã‚Šã¾ã™ã€‚")
        elif quality == 'good':
            recommendations.append("âœ“ è‰¯å¥½ãªå“è³ªã®ãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚")
        elif quality == 'acceptable':
            recommendations.append("âš ï¸ è¨±å®¹ç¯„å›²å†…ã®å“è³ªã§ã™ãŒã€ä»¥ä¸‹ã®æ”¹å–„ã‚’æ¨å¥¨ã—ã¾ã™ï¼š")
        else:
            recommendations.append("âŒ å“è³ªæ”¹å–„ãŒå¿…è¦ã§ã™ã€‚ä»¥ä¸‹ã®å¯¾ç­–ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ï¼š")

        # å€‹åˆ¥ã®æ¨å¥¨äº‹é …ã‚’è¿½åŠ 
        if web_validation.get('status') == 'validated':
            recommendations.extend(web_validation.get('recommendations', []))

        if youtube_validation.get('status') == 'validated':
            recommendations.extend(youtube_validation.get('recommendations', []))

        # è¬›åº§ç”Ÿæˆã¸ã®å½±éŸ¿ã‚’è©•ä¾¡
        total_data_points = integrated_summary.get('total_data_points', 0)
        if total_data_points > 0:
            recommendations.append(
                f"ğŸ’¡ {total_data_points}ä»¶ã®æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã‚’è¬›åº§ã®å…·ä½“ä¾‹ã¨ã—ã¦æ´»ç”¨ã§ãã¾ã™ã€‚"
            )

        return recommendations

    def save_report(self, report: Dict, output_path: str):
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ å“è³ªæ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {output_path}")
        except Exception as e:
            print(f"âœ— ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)

    def print_report_summary(self, report: Dict):
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("\n" + "=" * 60)
        print("ğŸ“Š è¬›åº§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªæ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 60)

        # ç·åˆå“è³ª
        quality = report.get('overall_quality', 'unknown')
        quality_icons = {
            'excellent': 'ğŸŒŸ',
            'good': 'âœ“',
            'acceptable': 'âš ï¸',
            'needs_improvement': 'âŒ',
            'unknown': '?'
        }
        quality_labels = {
            'excellent': 'å„ªç§€',
            'good': 'è‰¯å¥½',
            'acceptable': 'è¨±å®¹ç¯„å›²',
            'needs_improvement': 'æ”¹å–„å¿…è¦',
            'unknown': 'ä¸æ˜'
        }

        icon = quality_icons.get(quality, '?')
        label = quality_labels.get(quality, 'ä¸æ˜')
        print(f"\n{icon} ç·åˆå“è³ªè©•ä¾¡: {label}")

        # çµ±åˆã‚µãƒãƒªãƒ¼
        summary = report.get('integrated_summary', {})
        print(f"\nğŸ“ˆ çµ±åˆã‚µãƒãƒªãƒ¼:")
        print(f"  - ç·æƒ…å ±æºæ•°: {summary.get('total_information_sources', 0)}ä»¶")
        print(f"  - ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°: {summary.get('total_data_points', 0)}ä»¶")
        print(f"  - ä¿¡é ¼æ€§ã®é«˜ã„æƒ…å ±æº: {summary.get('credible_sources', 0)}ä»¶")

        volume = summary.get('total_content_volume', {})
        print(f"  - ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é‡:")
        print(f"    â€¢ Web: {volume.get('web_characters', 0):,}æ–‡å­—")
        print(f"    â€¢ YouTube: {volume.get('youtube_words', 0):,}èª")

        # æ¨å¥¨äº‹é …
        print(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
        for rec in report.get('quality_recommendations', []):
            print(f"  {rec}")

        print("=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description='è¬›åº§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªæ¤œè¨¼ãƒ„ãƒ¼ãƒ« - ãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚’ãƒã‚§ãƒƒã‚¯'
    )

    parser.add_argument(
        '--web-research',
        help='Webãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ï¼ˆJSONï¼‰ã®ãƒ‘ã‚¹'
    )
    parser.add_argument(
        '--youtube-research',
        help='YouTubeæ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ï¼ˆJSONï¼‰ã®ãƒ‘ã‚¹'
    )
    parser.add_argument(
        '--output',
        default='quality_validation_report.json',
        help='å“è³ªæ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: quality_validation_report.jsonï¼‰'
    )

    args = parser.parse_args()

    # å°‘ãªãã¨ã‚‚1ã¤ã®å…¥åŠ›ãŒå¿…è¦
    if not args.web_research and not args.youtube_research:
        parser.error("--web-research ã¾ãŸã¯ --youtube-research ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")

    print("=" * 60)
    print("ğŸ“‹ è¬›åº§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªæ¤œè¨¼ãƒ„ãƒ¼ãƒ« - å·¥ç¨‹1C")
    print("=" * 60)

    validator = CourseQualityValidator()

    # Webãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼
    web_validation = {'status': 'skipped'}
    if args.web_research:
        print(f"\nğŸ“š Webãƒªã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼ä¸­: {args.web_research}")
        try:
            with open(args.web_research, 'r', encoding='utf-8') as f:
                web_data = json.load(f)
            web_validation = validator.validate_web_research(web_data)
            print(f"  âœ“ {web_validation.get('summary', {}).get('total_sources', 0)}ä»¶ã®æƒ…å ±æºã‚’æ¤œè¨¼ã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
            web_validation = {'status': 'error', 'message': str(e)}

    # YouTubeæ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼
    youtube_validation = {'status': 'skipped'}
    if args.youtube_research:
        print(f"\nğŸ¥ YouTubeæ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼ä¸­: {args.youtube_research}")
        try:
            with open(args.youtube_research, 'r', encoding='utf-8') as f:
                youtube_data = json.load(f)
            youtube_validation = validator.validate_youtube_research(youtube_data)
            print(f"  âœ“ {youtube_validation.get('summary', {}).get('total_videos', 0)}ä»¶ã®å‹•ç”»ã‚’æ¤œè¨¼ã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
            youtube_validation = {'status': 'error', 'message': str(e)}

    # çµ±åˆå“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    report = validator.generate_quality_report(web_validation, youtube_validation)

    # ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
    validator.save_report(report, args.output)

    # ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    validator.print_report_summary(report)


if __name__ == '__main__':
    main()
